<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling</title>
        <link rel="icon" type="image/x-icon" href="favicon.ico">
        <link rel="stylesheet" href="fonts/avenir-next/stylesheet.css">
        <link rel="stylesheet" href="fonts/segoe-print/stylesheet.css">
        <link rel="stylesheet" href="icons/style.css">
        <link rel="stylesheet" href="css/window.css">
        <link rel="stylesheet" href="css/carousel.css">
        <link rel="stylesheet" href="css/selection_panel.css">
        <link rel="stylesheet" href="css/main.css">
        <link rel="stylesheet" href="css/bulma.min.css">

        <script src="js/window.js"></script>
        <script src="js/carousel.js"></script>
        <script src="js/selection_panel.js"></script>
        <script src="js/generation.js"></script>
		<script src="js/main.js"></script>
        <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
    </head>
    <body>

    <div id="main">
        <!-- Title -->
        <div id="title" class="x-gradient-font">
            <small></small>
            <img src="assets/images/logo.png" style="width:380px; height:auto;" alt="GarmageNet: "/> <br>
            <small><small><small><small>
            A Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling
            </small></small></small></small>
        </div>
        <!-- Authors -->
        <div id="authors">
            <div><a href="">Siran Li</a><sup>1,2,*</sup>,</div>
            <div><a href="">Chen Liu</a><sup>4,2,*</sup>,</div>
            <div><a href="https://walnut-ree.github.io/">Ruiyang Liu</a><sup>2,*&dagger;</sup>,</div>
            <div><a href="">Zhendong Wang</a><sup>2</sup>,</div>
            <div><a href="">Gaofeng He</a><sup>2</sup>,</div>
            <div><a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a><sup>3</sup>,</div>
            <div><a href="http://www.cad.zju.edu.cn/home/jin/">Xiaogang Jin</a><sup>4</sup>,</div>
            <div><a href="https://wanghmin.github.io/">Huamin Wang</a><sup>2</sup>,</div>
        </div>
        <div id="institution">
            <div><sup>1 </sup>Zhejiang Sci-Tech University,</div>
            <div><sup>2 </sup>Style3D Research,</div>
            <div><sup>3 </sup>Shanghai Jiao Tong University,</div>
            <div><sup>4 </sup>State Key Lab of CAD&CG, Zhejiang University</div>
            <br/><br/>
            <div><sup>* </sup>Equal contribution&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>&dagger; </sup>Corresponding author</div>
        </div>

        <div id="links">
            <div><a id="paper" href="https://arxiv.org/pdf/2504.01483.pdf">Paper</a></div>
            <div><a id="arxiv" href="https://arxiv.org/abs/2504.01483">Arxiv</a></div>
            <div><a id="code" href="https://github.com/Style3D/garmagenet-impl">Code</a></div>
            <div><a id="dataset" href="https://style3d.github.io/garmagenet/">Dataset</a></div>
        </div>

        <div id="teaser">
            <div style="width: 95%;">
                <img src="assets/images/garmage_teaser.png" alt="teaser image"/>
            </div> <br/>

            <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
                <b>GarmageNet in Action: </b>
                <small>
                A diverse and sophisticated collection of garment assets automatically generated by our <b>GarmageNet</b> framework, 
                along with their corresponding Garmages—our unified 2D–3D representation that encodes both sewing patterns and detailed geometry for seamless integration with existing garment modeling workflow. 
                Altogether, GarmageNet generates garments across the spectrum of design complexity: from intricate multi-layered ensembles (3rd and 5th) and striking asymmetric styles (2nd and 4th) to form-fitting corsets requiring precise drape and structural fidelity (1st).
                </small>
            </div>
        </div>

        <!-- Abstract -->
        <div id="abstract" class="x-gradient-block">
        <small>
        Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments.
        We introduce <i>GarmageNet</i>, a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. 
        Central to our approach is <i>Garmage</i>, a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment shapes. 
        <i>GarmageNet</i> employs a latent diffusion transformer to synthesize panel-wise geometry images and integrates <i>GarmageJigsaw</i>, a neural module for predicting point-to-point sewing connections along panel contours. 
        To support training and evaluation, we build <i>GarmageSet</i>, a large-scale dataset comprising over 10,000 professionally designed garments with detailed structural and style annotations.
        Our method demonstrates versatility and efficacy across multiple application scenarios, including <i>scalable garment generation</i> from multi-modal design concepts (text prompts, sketches, photographs), 
        <i>automatic modeling</i> from raw flat sewing patterns, <i>pattern recovery</i> from unstructured point clouds, and <i>progressive garment editing</i> using conventional instructions-laying the foundation for fully automated, production-ready pipelines in digital fashion.
            </small>
        </div>

        <!-- Overview -->
        <div class="x-section-title"><div class="x-gradient-font">Methodology</div></div>
        <p>
            <img src="assets/images/overview.png" alt="Pipeline of the method" style="width: 100%;">
        </p>
        <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
            <b>Framework Overview: </b>
            <small>
                Our framework seamlessly converts <b>multi-modal design inputs</b> (a), including text descriptions, sewing patterns, line-art sketches, and point clouds into <b>simulation-ready garment assets</b> (d). 
                Central to our framework is the novel <b>Garmage representation</b> (b), a unified 2D–3D structure encoding each garment as a structured set of per-panel geometry images. 
                Leveraging Garmage, our approach efficiently recovers <b>vertex-level sewing relationships</b> and <b>detailed 3D draping initializations</b> (c), enabling direct and high-quality garment simulation.
            </small>
        </div>

        <!-- Generation Pipeline-->
        <div class="columns is-vcentered" style="margin: 15px;">
            <div class="column is-half">
            <figure class="image">
                <img src="assets/images/garmage-pipeline.png" alt="Teaser Image">
            </figure>
            </div>
            <div class="column is-half">
            <h2 class="content has-text-left" style="font-weight: 500; color: #717388;">
                <b>Diffusive Garmage Generation.</b> 
                <small>
                During the <i>geometry encoding stage (top)</i>, each garment is encoded into a set of fixed-size (72-dimensional) latent vectors using a Variational Autoencoder (VAE). 
                These compact latent representations serve as training targets for the subsequent <i>diffusion generation stage (bottom)</i>. 
                <br/><br/>
                In the diffusion generation stage, we employ a diffusion transformer (DiT) denoiser, 
                integrating multi-modal conditions, including line-art sketches, raw sewing patterns, and point clouds via cross-attention mechanisms to effectively guide and control the garment generation process.
                </small>
            </h2>
            </div>
        </div>

        <!-- Stitching Pipeline-->
        <div class="columns is-vcentered" style="margin: 15px;">
            <div class="column is-half">
            <h2 class="content" style="font-weight: 500; color: #717388;">
                <b>Simulation-ready Sewing-Pattern Recovery.</b>  
                <small>  
                We sample boundary points (c) from the generated Garmage, classify them as sewing or non-sewing (d), and predict point-to-point stitches (e) as an adjacency matrix. 
                Vectorized sewing patterns (b) are extracted from the Garmage silhouette, stitched accordingly (f), and reconstructed into triangle meshes via Delaunay triangulation. 
                Vertex-wise draping from Garmage is then applied, yielding a simulation-ready mesh for integration into any cloth simulation engine to produce the final garment (g).
                </small>
                </h2>
            </div>
            <div class="column is-half">
            <figure class="image">
                <img src="assets/images/garmagejigsaw.png" alt="Teaser Image">
            </figure>
            </div>
        </div>

        <!-- GarmageSet Examples-->
        <div class="x-section-title"><div class="x-gradient-font">GarmageSet</div></div>
        <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
            <b>GarmageSet </b>
            <small>
            is a professionally curated, industrial-grade dataset comprises <b style="color: #8B645A;">14,801</b> unique garments spanning <b style="color: #8B645A;">five</b> 
            major clothing categories like tops, pants, skirts, dresses, outerwears 
            and several minor categories like bras, vests, pajamas. 
            </small>
        </div>
        <div id="results-garmageset"></div>

        <!-- Multi-modal Generation Examples-->
        <div class="x-section-title"><div class="x-gradient-font">Multi-modal Generation</div></div>
        <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
            <b>GarmageNet </b>
            <small>
            can generate 3D assets from various input modalities, including <b style="color: #717388;">text</b>, <b style="color: #717388;">line-art sketches</b> and <b style="color: #717388;">in-the-wild images</b>.
            </small>
        </div>


        <!-- Caption Generation Examples-->
        <div class="x-section-title"><div class="x-gradient-font" style="font-size: 24px">---> caption generation</div></div>
        <div id="results-txt2"></div>

        <!-- Image & Sketch Generation Examples-->
        <div class="x-section-title"><div class="x-gradient-font" style="font-size: 24px">---> image & sketch generation</div></div>
        <div id="results-sketch"></div>

        <!-- Automatic Garment Modeling Examples-->
        <div class="x-section-title"><div class="x-gradient-font">Automatic Garment Modeling</div></div>
        <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
            <small>
            In industry practice, garments typically exist as raw 2D sewing patterns (.DXF) without corresponding draped 3D assets.
            Converting these patterns into draped meshes is a complex, simulation-guided process that often requires multiple iterations and can take hours or even days for professionals.
            Leveraging a masked training scheme during latent encoding, <b>GarmageNet </b> generates complete garment assets from raw sewing patterns,
            by providing fine-grained 3D initialization through the Garmage representation and establishing vertex-level stitching relationships using <b>GarmageJigsaw</b>.
            </small>
        </div>
        <div id="results-autoGM"></div>

        <!-- Sewing Pattern Recovery Examples-->
        <div class="x-section-title"><div class="x-gradient-font">Sewing Pattern Recovery</div></div>
        <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
            <small>
            With the rise of 3D scanning and multi-view reconstruction, capturing realistic garment shapes as unstructured point clouds has become easier than ever. 
            But these raw 3D forms lack the <b>structured sewing patterns</b> needed for actual production. 
            <b>GarmageNet</b> bridges this gap by transforming point-cloud data of draped garments into Garmages with structured sewing patterns, enabling direct integration into manufacturing workflows.
            </small>
        </div>
        <div id="results-SPrecovery"></div>

        <!-- Progressive Generation And Editing Video -->
        <div class="x-section-title"><div class="x-gradient-font">Progressive Generation And Editing</div></div>
        <div class="x-center-text" style="text-align:justify; font-size: 16px; font-weight: 500; color: #717388;">
            <b>GarmageNet </b>
            <small>
            supports advanced garment editing functionalities, such as adding, deleting, or replacing components of an existing garment.
            </small>
        </div>
        <video autoplay controls muted loop height=" 20%" style="margin: 16px;">
            <source src="assets/demo_video_editing.mp4"
            type="video/mp4">
        </video>

            
        <div class="x-section-title"><div class="x-gradient-font">Citation</div></div>
        <p class="bibtex x-gradient-block">
@article{li2025garmagenet,
    title={GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling},
    author={Li, Siran and Liu, Chen and Liu, Ruiyang and Wang, Zhendong and 
            He, Gaofeng and Li, Yong-Lu and Jin, Xiaogang and Wang, Huamin},
    journal={arXiv preprint arXiv:2504.01483},
    year={2025}
}
        </p>
        
    </div>
        
    <div id="bottombar">
        <div class="row">
            <div>
                The page template is borrowed from <a href="https://microsoft.github.io/TRELLIS/"><span>T<span style="font-size: 10px;">RELLIS</span></span></a>.
            </div>
        </div>
    </div>

    </body>
</html>